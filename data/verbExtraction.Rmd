---
title: "Verb Extraction"
author: "Debbie Odufuwa & Jane Kwon"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(childesr)
library(tidyverse)
```

# Loading English Tokens

## All tokens for children and parents in Eng-NA & Eng-Uk Collections of CHILDES
```{r}
# Note: takes a few minutes to run
english_tokens <- get_tokens(collection = c("Eng-NA", "Eng-UK"),
                            role = c("target_child","Mother", "Father"),
                            token = "*")
```

# Token Processing

## Extracting Verb Tokens

### All verb tokens between 0-36 months (small dataset)
```{r}
verb_tokens_small <- english_tokens %>%
  filter(target_child_age >= 0 & target_child_age <= 36,
         part_of_speech == "v") %>%
  select(gloss, stem, target_child_age, speaker_role, target_child_id, utterance_id) %>%
  arrange(target_child_age)
verb_tokens_small
```

### All verb tokens between 0-72 months (large dataset)
```{r}
verb_tokens_large <- english_tokens %>%
  filter(target_child_age >= 0 & target_child_age <= 72,
         part_of_speech == "v") %>%
  select(gloss, stem, target_child_age, speaker_role, target_child_id, utterance_id) %>%
  arrange(target_child_age)
verb_tokens_large
```

### Saves verb token extractions to CSV file
```{r}
write.csv(verb_tokens_small, "../data/verb_tokens_small.csv", row.names = F)
```

```{r}
write.csv(verb_tokens_large, "../data/verb_tokens_large.csv", row.names = F)
```

## Extracting Untagged Tokens

### All tokens that have no tag between 0-36 months
```{r}
untagged_tokens_small <- english_tokens %>%
  filter(target_child_age >= 0 & target_child_age <= 36,
         part_of_speech == "") %>%
  select(gloss, stem, target_child_age, speaker_role, target_child_id, utterance_id) %>%
  arrange(target_child_age)
untagged_tokens_small
```

### All tokens that have no tag between 0-72 months
```{r}
untagged_tokens_large<- english_tokens %>%
  filter(target_child_age >= 0 & target_child_age <= 72,
         part_of_speech == "") %>%
  select(gloss, stem, target_child_age, speaker_role, target_child_id, utterance_id) %>%
  arrange(target_child_age)
untagged_tokens_large
```

### Helpful Info
```{r}
print(paste("Total Tokens: ", nrow(english_tokens)))

print(paste("Verb Tokens (0-36): ", nrow(verb_tokens_small)))
print(paste("Untagged Tokens (0-36): ", nrow(untagged_tokens_small)))
print(paste("Proportion Untagged Tokens (0-36): ",
            nrow(untagged_tokens_small) / nrow(english_tokens)))

print(paste("Verb Tokens (0-72): ", nrow(verb_tokens_large)))
print(paste("Untagged Tokens (0-72): ", nrow(untagged_tokens_large)))
print(paste("Proportion Untagged Tokens (0-72): ",
            nrow(untagged_tokens_large) / nrow(english_tokens)))
```

It isn't the case that a large portion of tokens in the data frame do not have
tags. So we did not go through and classify untagged verb tokens and add them 
to our verb data frame.


# CELEX Phonetic Encoding

The code below adds columns for the CELEX transcription/phonological encoding
of the stem and past tense used in Kirov and Cotterell (2018).
<!--https://github.com/ckirov/RevisitPinkerAndPrince/blob/master/experiment_1/english_merged.txt-->

## Reads in the CELEX Data Set
```{r}
celex_path <- "../data/english_merged.txt"
celex_dict <-
  read.table(celex_path, header = FALSE, sep = "\t", quote = "", fill = T,
             col.names = c("stem", "past_tense", "stem_celex",
                           "past_tense_celex", "reg_irreg"),
             stringsAsFactors = F)

celex_dict
```

## Merges CELEX info into verb_tokens_small by matching on the "stem" column
```{r}
verb_tokens_transcribed <-
  verb_tokens_small %>%
  left_join(celex_dict %>%
              select(stem, past_tense, stem_celex, past_tense_celex),
    by = "stem") %>%
  select(gloss, target_child_age, speaker_role, target_child_id,
         utterance_id, stem, stem_celex_encoding = stem_celex,
         past_tense, past_tense_celex_encoding = past_tense_celex)

verb_tokens_transcribed
```

## Saves new data frame to CSV file
```{r}
write.csv(verb_tokens_transcribed,
          "../data/verb_tokens_transcribed.csv", row.names = FALSE)
```

Use the verb_tokens_transcribed CSV file as input to celexToIPA.py (!)

# All Data Frames
```{r}
verb_tokens_small # N = 1218430 "verb" tokens
verb_tokens_large # N = 1869937 "verb" tokens
verb_tokens_transcribed # small dataset with CELEX encoding
verb_tokens_transcribed_ipa  # small dataset with CELEX+IPA encodings
```


# Updates / To Do

0.5. It seems each utterance ID is not unique. Redo code using gloss ID

1. Part of speech tags in CHILDES are not correct for all of the gloss and
some tokens are misclassified as verbs (big issue!!!)
  - Possible resolution method: Using the gloss ID for the verb, provide NLTK tagger
  with corresponding sentence containing the verb to provide part of speech tags
  and get the part of speech for the token. (Ensures that a token is a verb given the context)
  
  Ex) 
  gloss stem  target_child_age  speaker_role  target_child_id utterance_id
  dog   dog   3.460030          Target_Child  23465           17230451
  
```{r}
english_tokens %>% filter(gloss == "dog", target_child_id == 23465, utterance_id == 17230451)
```

2. Not every verb token in the CHILDES data frame that we have is in the list
of verbs used by Kirov and Cotterell (2018)
  - So some verbs in the final data frame do not have: celex encoding or past tense
  - Possible resolution method: Find a way to get past tense of verb and then do step 5.)

3. celexToIPA.py transcribes tokens using a different transcription scheme (CELEX2).
Currently have not found a way to get original transcription scheme used in paper.
  - See examples in Table 1 of the paper and compare with corresponding IPA 
  transcriptions in verb_tokens_transcribed.csv for reference
  
  Ours:   sing  sang  sɪŋ   saŋ
          go    went  ɡəʊ   wɛnt
  Kirov:  sing  sang  sɪŋ   sæŋ
          go    went  goʊ   wɛnt
  
4. Unclear whether Kirov and Cotterell (2018) & CELEX transcription uses
American or British IPA.
  - See "start", "determine" - no /ɹ/ sound transcribed. Could be a mix of the two Englishes.
  - Other examples: "know" and "go" transcribed with /əʊ/ (which is the British IPA vs /oʊ/ in American IPA)
  
5. For tokens without a CELEX encoding,
  1) Extract list of words without phonological encoding to new csv/txt
  2) Use a package to create ARPABET encodings of the words
  3) Use phonecodes python package to convert ARPABET encodings to IPA
  4) Merge into verb_tokens_transcribed_ipa to get complete dataset
  
  OR just use an IPA transcription tokenizer to transcribe all stems
  
6. Kirov paper also has a column for the past particple. Do we need to consider this as well?
